{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%autowait` not found.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR | 2023-01-26 17:09:26 | 835300370 | 55 | 'NoneType' object has no attribute 'select'\n",
      "ERROR | 2023-01-26 17:09:27 | 835300370 | 105 | ResultSet object has no attribute 'select_one'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ci_price': '', 'ci_incidence': 0, 'ci_incidence_specific_gravity': 0.0, 'ci_participation': 0, 'ci_participation_specific_gravity': 0.0}] {'ci_competition_rate': '', 'ci_promise_content': '', 'ci_promise_rate': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "project_path = str(Path.cwd().parent)\n",
    "sys.path.append(project_path)\n",
    "\n",
    "from typing import List, Dict, Union, Tuple\n",
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from apps.ipo.agents import get_user_agents\n",
    "from config.config_log import logging\n",
    "import requests\n",
    "\n",
    "logger = logging.getLogger(\"info-logger\")\n",
    "\n",
    "\n",
    "async def extract_data_from_table1(table: BeautifulSoup) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Extracts data from the first table in the HTML page and returns a list of dictionaries, where each dictionary\n",
    "    represents a row of data, with keys as the data categories and values as the data for each category.\n",
    "\n",
    "    Parameters:\n",
    "    - soup (BeautifulSoup): The BeautifulSoup object representing the HTML page.\n",
    "\n",
    "    Returns:\n",
    "    - List[Dict[str, str]]: A list of dictionaries containing the extracted data.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        keys = [\n",
    "            \"ci_price\",\n",
    "            \"ci_incidence\",\n",
    "            \"ci_incidence_specific_gravity\",\n",
    "            \"ci_participation\",\n",
    "            \"ci_participation_specific_gravity\",\n",
    "        ]\n",
    "        results = []\n",
    "        trs = table.select(\"tr\")[2:-1]\n",
    "        for tr in trs:\n",
    "            tds = tr.select(\"td\")\n",
    "            empty_string = tds[0].text.strip().replace(\" \", \"\")\n",
    "            if not empty_string:\n",
    "                break\n",
    "            temp = []\n",
    "            for td in tds:\n",
    "                temp.append(td.text)\n",
    "            results.append(temp)\n",
    "\n",
    "        result = [dict(zip(keys, result)) for result in results]\n",
    "\n",
    "        return result\n",
    "    except AttributeError as err:\n",
    "        logger.error(err)\n",
    "        result = [\n",
    "            {\n",
    "                \"ci_price\": \"\",\n",
    "                \"ci_incidence\": 0,\n",
    "                \"ci_incidence_specific_gravity\": 0.0,\n",
    "                \"ci_participation\": 0,\n",
    "                \"ci_participation_specific_gravity\": 0.0,\n",
    "            }\n",
    "        ]\n",
    "        return result\n",
    "\n",
    "\n",
    "async def extract_data_from_table2(table: BeautifulSoup) -> Dict[str, Union[str, float]]:\n",
    "    \"\"\"\n",
    "    Extracts data from the second table in the HTML page and returns a dictionary with keys as the data categories\n",
    "    and values as the data for each category.\n",
    "\n",
    "    Parameters:\n",
    "    - soup (BeautifulSoup): The BeautifulSoup object representing the HTML page.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, Union[str, float]]: A dictionary containing the extracted data.\n",
    "    \"\"\"\n",
    "    result = {\"ci_competition_rate\": \"\", \"ci_promise_content\": \"\", \"ci_promise_rate\": 0.0}\n",
    "    if not table:\n",
    "        return result\n",
    "    try:\n",
    "        result = {\n",
    "            \"ci_competition_rate\": table.select_one(\"tr:nth-of-type(1) > td:nth-of-type(2) > font > strong\").get_text().replace(\" \", \"\").replace(\"\\xa0\", \"\"),\n",
    "            \"ci_promise_content\": table.select_one(\"tr:nth-of-type(2) > td:nth-of-type(2)\").get_text().strip(),\n",
    "            \"ci_promise_rate\": table.select_one(\"tr:nth-of-type(3) > td:nth-of-type(2)\").get_text().strip()\n",
    "        }\n",
    "        return result\n",
    "    except (AttributeError, IndexError):\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "async def scrape_ipostock(code: str) -> Tuple[List[Dict[str, str]], Dict[str, Union[str, float]]]:\n",
    "    \"\"\"\n",
    "    Scrapes data from the webpage for the given stock code and returns a tuple of the extracted data from the two tables on the page.\n",
    "\n",
    "    Parameters:\n",
    "    - code (str): The stock code of the company.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[List[Dict[str, str]], Dict[str, Union[str, float]]]: A tuple containing the extracted data.\n",
    "    \"\"\"\n",
    "    url = f\"http://www.ipostock.co.kr/view_pg/view_05.asp?code={code}\"\n",
    "    header = await get_user_agents()\n",
    "    try:\n",
    "        # async with aiohttp.ClientSession() as session:\n",
    "        #     async with session.get(url, headers=header) as resp:\n",
    "        #         soup = BeautifulSoup(await resp.text(), \"lxml\")\n",
    "        # async with aiohttp.ClientSession() as session:\n",
    "            # async with session.get(url, headers=header) as resp:\n",
    "        resp = requests.get(url, headers=header)\n",
    "        soup = BeautifulSoup(resp.content, \"lxml\")\n",
    "    except (aiohttp.ClientError, asyncio.TimeoutError) as e:\n",
    "        print(\"Request failed, retrying in 5 seconds...\")\n",
    "        print(e)\n",
    "        await asyncio.sleep(0.3)\n",
    "    table1 = soup.find(\"table\", width=\"780\", cellpadding=\"0\", class_=\"view_tb\")\n",
    "    table2 = soup.find_all(\"table\", width=\"780\", cellspacing=\"1\", class_=\"view_tb2\")\n",
    "\n",
    "    t1, t2 = await asyncio.gather(\n",
    "        extract_data_from_table1(table1),\n",
    "        extract_data_from_table2(table2),\n",
    "    )\n",
    "    print(t1, t2)\n",
    "    return t1, t2\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #print > table > tbody > tr:nth-child(5) > td > table.view_tb > tbody > tr:nth-child(3) > td:nth-child(1)\n",
    "    #\"print > table > tr:nth-child(5) > td > table.view_tb > tr:nth-child(3) > td\"\n",
    "    async def main():\n",
    "\n",
    "        #        code = \"B202010131\"\n",
    "        # code = \"B202010131\"\n",
    "        code = \"B202105031\"\n",
    "        # code = '유안타제12호스팩'\n",
    "        code = \"B202211161\"\n",
    "        # 비트나인\n",
    "        # code = \"B202104292\"\n",
    "        prediction_result, general_result = await scrape_ipostock(code)\n",
    "\n",
    "        from schemas.general import GeneralCreateSchema\n",
    "        from schemas.prediction import PredictionCreateSchema\n",
    "\n",
    "        g = GeneralCreateSchema(**general_result)\n",
    "        s = [PredictionCreateSchema(**data) for data in prediction_result or []]\n",
    "\n",
    "        # print(g)\n",
    "        # print(s)\n",
    "        # from pprint import pprint as pp\n",
    "\n",
    "        # # pp(g.dict())\n",
    "        # # pp(g.dict()[\"ci_competition_rate\"])\n",
    "        # # pp(g.dict()[\"ci_promise_rate\"])\n",
    "        # # pp(g.dict()[\"ci_promise_content\"])\n",
    "        # si1 = s[0].dict()\n",
    "        # si2 = s[1].dict()\n",
    "        # si3 = s[2].dict()\n",
    "        # si4 = s[3].dict()\n",
    "        # print(si1)\n",
    "        # print(si2)\n",
    "        # print(si3)\n",
    "        # print(si4)\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " <table bgcolor=\"D2D2D2\" bordercolor=\"D2D2D2\" cellpadding=\"1\" cellspacing=\"1\" class=\"view_tb2\" style=\"margin:10px 0px 0px 0px;\" width=\"780\">\n",
       " <tr align=\"left\" bgcolor=\"#FFFFFF\">\n",
       " <td bgcolor=\"F0F0F0\" width=\"132\">  <font color=\"#666666\">참고사항</font></td>\n",
       " <td height=\"26\" style=\"line-height:130%; padding:3px 3px 3px 3px;\" width=\"*\"><font color=\"red\">*금번 수요예측에 참여하는 기관투자자는 15일, 1개월, 3개월, 6개월의 의무보유기간을 확약가능.</font></td>\n",
       " </tr>\n",
       " </table>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code = \"B202211161\"\n",
    "url = f\"http://www.ipostock.co.kr/view_pg/view_05.asp?code={code}\"\n",
    "header = await get_user_agents()\n",
    "try:\n",
    "    # async with aiohttp.ClientSession() as session:\n",
    "    #     async with session.get(url, headers=header) as resp:\n",
    "    #         soup = BeautifulSoup(await resp.text(), \"lxml\")\n",
    "    # async with aiohttp.ClientSession() as session:\n",
    "        # async with session.get(url, headers=header) as resp:\n",
    "    resp = requests.get(url, headers=header)\n",
    "    soup = BeautifulSoup(resp.content, \"lxml\")\n",
    "except (aiohttp.ClientError, asyncio.TimeoutError) as e:\n",
    "    print(\"Request failed, retrying in 5 seconds...\")\n",
    "    print(e)\n",
    "    await asyncio.sleep(0.3)\n",
    "table1 = soup.find(\"table\", width=\"780\", cellpadding=\"0\", class_=\"view_tb\")\n",
    "table2 = soup.find_all(\"table\", width=\"780\", cellspacing=\"1\", class_=\"view_tb2\")[-1]\n",
    "table1, table2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d63fa3fafd5b69ac1a17fd48e1518475e26bb632168b41c435fb31415c5f1576"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
